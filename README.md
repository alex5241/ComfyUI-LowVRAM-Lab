# ComfyUI-LowVRAM-Lab
Experiments and optimizations for running ComfyUI on low-VRAM GPUs.

## 项目简介
本仓库是我在低显存显卡（4060Ti 8GB）上使用 ComfyUI 进行AI创作的笔记记录。

- 我在使用gpt或者deepseek解决comfyui相关的技术问题时，经常得到比较模糊或者不准确的回答。
- 我觉得可能是相关技术还比较新，而且各种大模型每月也都在更新，网络上还没有足够多的数据供给AI来抓取。
- 所以我创建了这个项目来自己记录。
- 本项目所有内容均为真实+原创+手打，希望我的项目对你有帮助。
- 本项目内容多为总结性质经验内容，并非新手教学方案。
- 本项目长期更新。但由于测试过程比较慢，更新不定期。

## 我的硬件配置
- GPU：RTX 4060Ti 8GB
- CPU：12600kf
- RAM：32GB
- SSD: 1T
- 系统：Windows 11

## 我的comfyui配置
- 整合包：秋叶整合包 
- 版本号：ComfyUI-aki-v1.7 （大小：4G左右）

## 一些通用的话
- 我的这套配置，在玩视频大模型，比如wan系列的时候，原模上都是无法跑起来的。
- 通常意义上，社区都会有量化或者蒸馏模型，比如gguf或者lightx2v系列。他们能让低配置的机器跑起来。
- 很多人以为只需要显存大就行，实际上内存也很重要，我32g物理内存+32g虚拟内存，经常占用双100%。长期跑AI，尤其是视频，建议48g内存起步。
- 大显存比高级别显卡重要。比如5060ti 16g，优于5070 12g.很多模型显存大了才能跑。
- 经常有显卡一样的人跑一套工作流，一个跑不了，一个能跑。可能就是一个内存大，显存通过block swap,用内存顶了一下。
- 我的配置也能玩，但是大多数情况下都是在降低分辨率或者花费更多等待时间，出来的效果也很一般。如果你想玩的爽，我不推荐我的配置。
- 我个人推荐的入门硬件配置是：intel cpu + 显卡16g + 内存 48g + 固态 2T 。这只是入门，如果没到这个配置也能玩，就像我一样。
- wan2.2 在技术上完全替代wan2.1，我现在已经删除了wan2.1的所有模型及工作流。
- wan2.5 风声大雨点小，10.1前开放的是收费模式，可能后期不会再开源。目前都在等消息。
- 如果现成的业务需要AI快速出东西，我建议直接在云平台如RH上玩，因为本地搭建环境还挺复杂的，尤其是对于非技术人员。
- 云平台的好处是不用搭建环境，直接就能跑。坏处是不能nsfw；需要付费；需要排队等待。本机跑的好坏反之。
- 社区里有很多做融合，加速等模型的，基本上每周都会出新模型，迭代速度非常快。comfyui也基本每天都会有新的内容更新。
- 目前社区的活跃度是非常高的，各种群里的消息基本动不动就在上千条。
- b站有很多卖课的，到处搬运别人的视频或者工作流，筛选内容要睁大眼睛。

## 关于环境：
- 整合包有自己的一套环境，都在整合包里。与电脑安装的python，torch等环境无关。
- 秋叶整合包目前最稳定的版本是v1.7。内置的是 torch 2.7.0+cu128 ,  python 3.11.
- 秋叶v2版本过高，部分python库或者节点库等没跟上适配，会有莫名其妙的问题，比如卡采样器，就是加速失效导致。
- 其他的整合包也大差不差，有的很大很全，内置了所有的节点和模型库，文件就会很大，20g的整合包也经常见。
- 整个加速依赖关系为  cuda->python->pytroch->xformers或sage（要依赖triton）或flashattention，各依赖之间必须版本匹配一致。
- 自行安装或者升级环境时，循序上面的依赖关系，并严格保持版本匹配进行下载安装,下方为一个比较稳定的版本示例：
  - pip list ,  python 3.11   
  - 先下python官网非exe的集成包，其内部没有pip，再加上pip，然后可用pip命令下载其他包。
  - pytorch  必须下带有cuda加速包 ,   一般整合包自带   2.7.0+cu128
  - xformers  命令行安装   0.0.30+4cf69f09.d20250606
  - triton 命令行安装  pip install -U "triton-windows<3.4"    triton-windows-3.3.1.post19 ，实际下载的是 triton_windows-3.3.1.post19-cp311-cp311-win_amd64.whl
  - sageattention2,  下载对应版本，本地安装。  2.1.1+cu128torch2.7.0
- 下载多版本选择时，windows/linux是平台，cp是python版本,tor是torch缩写，cu是cuda缩写，x86,arm是cpu类型,32 64是cpu位数，现在基本都是64位，都是x86_64,有些是amd64。
- 工作流层面的常用加速：
  - 双节棍  需要安装python库 https://github.com/nunchaku-tech/nunchaku/releases 和节点 https://github.com/nunchaku-tech/ComfyUI-nunchaku 同时需配合自己的大模型,所以每有一个大模型，双节棍就得基于官方出自己的模型库；
  - lightx2v: 蒸馏模型，需配合它自己的蒸馏模型或者lora ；
  - teacache 不用；
  - 其他自带优化加速的模型。
- 单独安装python包，需要在启动器内部打开命令行， python命令改为python.exe -m + pip命令 。  比如： pythone.exe -m pip install
- 基本上所有东西安装就三种，都是往model里扔东西，会了一个就都全会了：
  - 直接拷贝内容到对应目录即可。
  - 内容需要对应节点一起用，安装对应节点。内容可能不是一个，是多个； 或者内容需要py脚本等一堆，直接全下载拷进去。
  - 内容需要底层python库支持，先安，再下节点，再下内容。比如双节棍。

## 一些奇怪的问题
- 采样器卡主了，长时间不动，也不会有报错，超长20分钟等待后，完成运行。 这种大概率是环境版本不匹配，导致加速无法开启。切到py311后正常。
- 环境变量： 变量名：PYTORCH_FLOAT32_MATMUL_PRECISION  变量值：high  ，改了一次没生效。再改后没发现有啥变化。
- 加载lora经常打印出长串不匹配，通常是底膜和lora兼容问题，不影响运行，但是可能lora会失效。
- 使用SeedVR2放大时，报错提示: The api key cient option must be set either by passing api key to the cient or by setting the OPENAI AP!KEY environment variable
  请在路径ComfyUl custom nodesl ComfyUl-SeedVR2 VideoUpscaler-sageattn文件夹内修改 /src/core/generation.py,
  src/interfaces/comfyui node.py 里的所有show tensors=False，所有 true 的改成 false，本来是 false 就不用管


## 关于wan2.2
- 我的配置是无法跑wan2.2官方模型的 ，只能去跑gguf，在牺牲一定的精度后换取到了本地的可运行。
- 我本地跑的是gguf q4，我觉得我应该能跑到q5。
- 我本地是不跑KJ的所有工作流的。根据我的不充分测试，KJ的工作流一般比官方流更吃机器配置，且需要更大的内存。
- 视频分辨率尽量按照576/720/832/1024/1280这些尺寸来设定，有些模型不是这些尺寸会报错。 我常用的是 832*480
- ovi模型是声音驱动出视频的模型。
- 目前比较常用的方案是 锁死帧率16进行ai创作。在视频可行的前提下进行放大和插帧，生成更高清的视频。
- 我的机器用wan2.2 跑5s 832*480 图生视频 ，跑wan2.2 animate 832*480 5s视频替换，用时接近，一般都在3-5分钟。
- 为方便计算，我通常会粗略认为： ai要跑1s视频，基本要花费1分钟。
- 我目前常用的视频创作手法是： 先用文生图进行首尾帧的制作，然后再让wan根据首尾帧，结合描述词进行生成视频。
- 单凭描述词或者一张图进行视频生成时，ai抽卡很严重，经常跟我们想要的效果不一致，所以用首尾帧来控制ai的走向。

## 关于wan2.2 animate
- wan 是用于文或者图生视频的，自由度比较高。animate更多的是整活类，二创，人物，服装等替换类，等同于wan2.1的vace.
- wan animate可以完成视频换装，换脸，整人替换，动作迁移，人物迁移，局部替换，多人中指定人物替换等。
- 在wan animate工作流中，此时的提示词基本无用。

## 关于遮罩
- 自动版方案： unethuman，该方案对局部支持不如 sam2.1+grouded，后者直接可以写单词指定去除的局部，比如face ,dress之类的）  
- 手动版本：自己画遮罩。 低配可以用sam2 ，高配使用 sec4b 。

## 关于新遮罩模型sec4b，号称碾压sam2。
- 最开始需下载4个模型文件，共20g左右。 https://huggingface.co/OpenIXCLab/SeC-4B/tree/main 
- 后来有了单模型下载，且有fp8支持。https://huggingface.co/VeryAladeen/Sec-4B/tree/main
- 模型需要搭配配套comfyui节点一起使用。 https://github.com/9nate-drake/Comfyui-SecNodes
- 模型放置，节点文档说的是models/sams/sec-4b/，但我本地这样放置不对，我的位置在models\sams。
- 经测试，即便是fp8模型，我的8g显卡也跑不起来。运行工作流 load sec 会直接挂掉。这也符合文档说明，文档建议最低12g显存。
- 模型开发者称： fp8 模型发现有问题 ，comfyui节点已经删除对fp8支持，建议使用 fp16。
- 社区有更省显存和更快速度的节点。 https://github.com/lihaoyun6/ComfyUI-SecNodes_Ultra_Fast。
- 经测试，ComfyUI-SecNodes_Ultra_Fast节点搭配fp16模型在我本地可以完美运行。然后在wan2.2 animate工作流里也正常运行。

## 关于注意力加速
- 我一直没完全弄懂注意力加速到底该怎么开，但是我经常出问题的时候，就切一下就正常了。
- 图片处理开成xformers； 视频处理改成sageattention。
- 在模型都正常的情况下，如果采样器全黑，优先考虑切换到xformers试下。
- 很多视频工作流里是有切换到sageattention的内容的，运行时，会自动切换，不用在启动器里改动也可以。
- flash_attention_2： 只有linux版本,windows下无需关心。
- Flash Attention 仅在 FP16 / BF16 精度下可用，对于 FP32 / FP8 模型，README 明确写：“Automatically disabled for FP32/FP8 precision 。所以低配置电脑因为跑不起fp16的话，无需考虑。

## 关于提示词
- AI视频制作，核心理念就是：“图定逻辑，Wan出流动”。换句话说：用图（关键帧）定义内容与逻辑；用 Wan让它自然流动成视频。
- “分镜（storyboard prompt）” ≠ “关键帧（keyframe） ，前者关注的是镜头变化，后者是技术术语。
  - 硬分镜： 先分镜让qwen去生成多张首尾帧，通过首尾帧让wan2.2生成视频。
  - 软分镜： 给wan2.2 的提示词里 ，直接写上不同阶段镜头内容，让它去绘制。 这种可控性差。
- 去掉分段标签（第一阶段、第二阶段）——这类标签容易让模型“重置画面语义”，每段重新生成一次。
  - 加入连续连接词：“随着时间的推移”“逐渐”“一点点”“不断”——让语义在模型的时间潜空间中保持连续。
  - 添加环境保持词：“镜头始终固定”“画面连贯自然”“光线柔和”——帮助模型在不同帧中保持一致的光线与视角。
  - 避免突变动作词（比如“开始”“出现”）太密集，改成“缓缓”“逐渐”。——模型更倾向平滑变化。
  - 单场景、渐变式的视频，用的不是“分镜”，而是：时间递进式 prompt（temporal evolving prompt）也就是告诉模型“画面在变化”，而不是“镜头在切换”。
- 分镜（Storyboards / Scene Prompts）主要用于：
  - 镜头切换（办公室 → 街头 → 屋顶）   视角变化（远景 → 特写）
  - 动作变化（走 → 停 → 回头）     多场景连续叙事（剧情类视频）
  - 在这些场景下，你需要在 prompt 里写清楚：【第一镜头】、【第二镜头】、【第三镜头】
  - 或者在工作流中，用 不同 prompt 段落控制不同时间区段。
  - 在 Wan 2.2 或类似的时序生成模型里，如果你要表现“画作从无到有”的变化，仅靠一句长 prompt（哪怕写得再精细）模型都很难「理解出时间逻辑」，
  - 因为它不是强因果模型，只是按空间变化去“猜测运动”。
  -“笔触驱动显现”的因果关系。 wan2.2是根据前后帧的视觉变化预测中间帧”，而不是“根据物体动作（画笔）去驱动另一物体（画作）的变化”
- 目前写提示词比较好用的办法是告诉gpt或者豆包等，让他们直接给你生成提示词。同时要告诉他们，镜头切换，视角，人物，姿势，等。细节越多，效果越符合预期。
- 可以在gemini等中对其进行角色设定，然后再让其按照内容去生成提示词，效果会比较好。


## 常用网址

## License
MIT License
转载请注明出处

## 项目地址：https://github.com/alex5241/ComfyUI-LowVRAM-Lab
